# GitHub Copilot: A Threat to High School Security?
## Exploring GitHub Copilot’s Proficiency in Generating Malware from Simple User Prompts

**Eric Burton Martin, Sudipto Ghosh**  
_Department of Computer Science_  
_Colorado State University_  
_Fort Collins, CO, USA_  

### Abstract
This paper examines the potential implications of script kiddies and novice programmers with malicious intent having access to GitHub Copilot, an artificial intelligence tool developed by GitHub and OpenAI. The study assesses how easily one can utilize this tool to generate various common types of malware ranging from ransomware to spyware, and attempts to quantify the functionality of the produced code. Results show that with a single user prompt, malicious software such as DoS programs, spyware, ransomware, trojans, and wiperware can be created with ease. Furthermore, uploading the generated executables to VirusTotal revealed an average of 7/72 security vendors flagging the programs as malicious. This study has shown that novice programmers and script kiddies with access to Copilot can readily create functioning malicious software with very little coding experience. This paper discusses how this could potentially lead to an increase in internal attacks on schools due to the average age demographic of the target group. However, if used correctly this technology could potentially help this same demographic gain the skills needed for ethical hacking practices utilized in the cybersecurity space.

**Index Terms** — GitHub Copilot, cybersecurity, malware generation, school network security, internal cyber-attacks, cybersecurity risk management in schools, script kiddies

### I. Introduction
In 2021, the advent of GitHub Copilot [1], an innovative cloud-based artificial intelligence (AI) tool by GitHub and OpenAI, provided an eye-opening glimpse into the extraordinary capabilities of natural language processing AI trained on the vast wealth of the largest online code repository, GitHub. But remarkable power demands great responsibility, especially when wielded by the inexperienced. Primarily designed to augment code productivity among developers, this tool inadvertently provides novice hackers, or script kiddies [2], an unprecedented ability for generating malicious software.

Script kiddies – aspiring fledgling hackers lacking in advanced technical prowess but motivated to find and utilize ready-made scripts and malicious programs from the internet – have historically inflicted significant damage on vulnerable systems [2], [3]. We hypothesize that Copilot’s arrival can potentially supercharge their arsenal, enabling them to craft complex, customized malware with alarming ease. This powerful tool thus stands at the precipice of a double-edged sword, capable of creating new threat landscapes if used by novice programmers and script kiddies who may not fully comprehend the ethical and legal implications of their actions.

A noteworthy demographic factor intensifies this situation: many of these beginners are under the age of 18. These individuals, still on their journey of personal and intellectual maturity, may not completely understand the gravity of their actions [4]. This could lead to them misusing Copilot to generate malicious code and experimenting with it in their school networks, personal home networks, or online, neglecting the possible widespread impact of their actions.

In this paper we undertake an exploratory assessment of Copilot’s potential for misuse, putting ourselves in the shoes of an amateur programmer or script kiddie. By adopting this distinctive perspective, we evaluate the tool’s capabilities for generating typical malware types—from ransomware to spyware. We attempt not only to mirror their potential actions but also to evaluate the real-world applicability of the produced code. The insights from the study are quite alarming, unearthing potential gaps in our preparedness for such emerging threats. These findings underscore the urgent need to bolster cybersecurity awareness and build sturdy defense mechanisms, especially in high school environments. As we navigate this evolving landscape swarming with AI-assisted threats, it is crucial for educational institutions to prepare for a potential rise in internal attacks spearheaded by their student coders.

## II. METHODOLOGY

The objective of this study was to evaluate the hypothesis that with the advent of Copilot, novice programmers, and script kiddies will be able to create malicious software that may have previously been infeasible. After feeding a single prompt to Copilot in the VS Code environment, we repeatedly pressed the `Tab` and `Enter` keys until no new suggestions emerged. In some scenarios, we used the `CTRL+ENTER` command to access and select a specific solution recommended for a given prompt. When Copilot stopped generating code suggestions, we employed PyInstaller [5] to package the Python scripts into a Windows executable file. To minimize risk, no code was executed within VS Code. Instead, scripts were packaged and converted to an executable file. This file was then executed in a secure Windows 11 Pro Sandbox environment to evaluate if the malware operated as anticipated. We also uploaded the executables to VirusTotal for inspection by more than 70 antivirus scanners [6].

For this study, we chose Python as the coding language given its easy-to-learn syntax and wide acceptance among novice programmers, a perspective we aimed to replicate. While C and C++ are often preferred for malware creation, Python has been used in numerous malware instances [7]. We expect that script kiddies would prefer Python to C/C++ for its straightforward execution and lower learning curve.

### A. Generated Malware Types

Each of the chosen malware types represents a different type of cybersecurity threat, allowing for a comprehensive exploration of potential misuse of AI-based coding tools like Copilot. Together they represent a broad spectrum of threats that novice programmers might generate.

- **Ransomware**: denies the victim’s data access via encryption until a ransom is paid.
- **Spyware**: software monitors infected users’ activities. For this study, keyloggers, video camera recording, and audio listener malware were created.
- **Trojans**: disguise themselves as a desirable piece of code. In this study, malware was injected within PDFs and Microsoft Word documents.
- **Worms**: spread from one computer to another, typically over a network. Unlike viruses, worms can replicate independently without requiring a host program. In this study, we attempt both network and file-to-file proliferation.
- **Wiperware**: erases user data irretrievably by deleting and overwriting the deletions with random bytes to further hinder document recovery.
- **Denial of Service attacks**: in this study focused on computer-specific forms of DoS, including keyboard, mouse, memory, and screen attacks, rather than network-based attacks.

### B. Metrics

We gathered both qualitative and quantitative data to evaluate the performance and efficiency of Copilot.

- **Prompts**: The specific prompt or prompts used to generate each piece of malware are recorded.
- **Cycles**: Documented per malware per occurrence, including how they were corrected.
- **Execution Errors**: The types of errors encountered during the packaging of the scripts were recorded.
- **Corrections Needed**: When execution errors were encountered, we attempted to rectify these and recorded the difficulty level of the required corrections.
- **Cognitive Complexity**: Obtained using Code Climate [8].
- **Lines of Code**: We tracked the lines of code generated.
- **Descriptive Comments**: We assessed Copilot’s capability to generate descriptive comments for the lines or blocks of code it produced.
- **Functionality**: The functionality of the generated code was evaluated based on how well each generated malware script fulfilled the base requirements of their corresponding initial prompt.
- **Antivirus Detection**: The created executables were uploaded to VirusTotal to gauge their detection rate by over 70 antivirus scanners.

## III. RESULTS (See Paper)

### IV. DISCUSSION

Despite not using GitHub Copilot as a copilot but rather using it as a pilot during the coding process, our study highlighted its potential misuse in generating malware. If leveraged by a skilled hacker, the ramifications could be profound.

We initiated each malware category with a simple prompt, letting Copilot take the lead. Approximately half of the time, Copilot suggested extensions to the initial prompt. A common issue was Copilot entering a cyclical suggestion loop, where it continually offered previously provided or implemented suggestions. Interestingly, Copilot seemed to reference all the code in the current working directory, even suggesting methods and code from other files. To avoid redundancy and the predisposition to suggest already written code, we created each new file in a separate directory.

Regarding errors encountered when attempting to execute each program, the most common error was a trivial dependency error. Syntax errors were quite uncommon and oftentimes occurred when the suggestion cut itself off. In one of the 29 programs, Copilot began polluting the namespace and passing in a function parameter with the same name as a previously defined global variable within the function causing an error due to the parameter variable being assigned before the global declaration. Other errors encountered were type errors where Copilot was passing the incorrect data type into method calls. We did not end up categorizing any corrections as ‘Complex’ since the programs that tended to have obscure errors were the most complex programs that we deemed infeasible to address considering the scope of this project. We categorized the difficulty to correct such cases as ’Unknown’.

The created malware’s functionality was impressive. Around 60% of the malware performed as intended. The easiest class of malware to create was spyware. It could surreptitiously activate the user’s webcam and microphone, save recordings to a disguised file in the user’s public documents, and be executed in under a minute. For example, a video file was saved as `1670194934.8706307` that contained a recording of the user. The DoS programs were the second most successful malware class. The generated scripts would prevent people from doing anything on their laptops by either moving the mouse to the top left corner of the screen every millisecond or creating a screen that cannot be minimized or exited since it continuously is called to the front of the screen.

The ransomware and wiperware had a lower success rate than spyware but they were successful in their operation of wiping or encrypting directories. When creating these, we limited the scope to a single directory due to security concerns. The ransomware encrypted the files and the wiperware overwrote and deleted them. The trojans generated performed the base functionality of a trojan since they successfully opened documents and injected code inside them but the code did not execute as defined in the prompts when the tampered files were opened. The worms were the least successful malware class generated; this may be because they are often network crawlers which are far more complex than the other malware created in this study. The one successful worm we created is a very basic toy worm that creates a copy of itself in a directory and then runs that copy.

Despite the successful creation of various types of malware, an average of only 7/72 vendors on VirusTotal flagged these scripts as malicious. Our hypothesis as to why this low detection rate occurred is multifaceted. Firstly, our malware lacked active inbound and outbound network connections, which are typical characteristics of many forms of malicious software. Without such network activities, the scripts might not trigger the detection mechanisms of several antivirus software. Secondly, the malware scripts generated were relatively simple and did not use sophisticated obfuscation or evasion techniques that are common in real-world malware. This simplicity might have led to these scripts being overlooked as non-malicious. Thirdly, the code produced by GitHub Copilot may have appeared benign due to its origin from public code repositories. Antivirus systems might not yet be designed to recognize this new form of code generation as potentially harmful. Upon rescanning the scripts a few days later though, we noticed an increase in detection. This suggests that some vendors may use uploaded data to VirusTotal to update their databases with new malware signatures, improving their detection capabilities over time.

## V. THREATS TO VALIDITY

The following are possible threats to the validity of this study.

- **Tool Expertise:** Novice programmers might not be adept at using VS Code, thereby introducing a learning curve that was not accounted for in this study. This study also assumes prior access and familiarity with Copilot.

- **Subjective Metrics:** Opinion-based metrics used in this study might not accurately reflect the difficulty level associated with various tasks, as perceptions may vary among individuals.

- **Variations in Malware Complexity:** This study might not fully capture the challenges and complexities that arise with more advanced, customized malware other than the relatively simple toy sized ones we used.

- **Limited Malware Categories:** The results may not generalize to malware types that were not included in the study.

- **Single-User Bias:** The findings might reflect individual biases in interpretation and execution of the tasks, which could limit the generalizability of the study.

- **Environmental Constraints:** This study did not fully address the impact of different operating environments, network settings, or security defenses on the malware functionality.

## VI. RELATED WORK

Our study of GitHub Copilot’s potential misuse aligns with several key works. Botacin [10] parallels our approach of dissecting malware creation and emphasizes the need for ongoing AI risk evaluation. Kshetri [11] discusses real-world misuse of LLMs like ChatGPT, underlining our own findings of GitHub Copilot’s cybersecurity risks. Weidinger et al. [12] emphasizes the lowered barrier for complex malware development using tools like Copilot, reinforcing the broader security implications our study uncovers. Lastly, Taddeo et al.’s work [13] on socio-technical ecosystem considerations in AI and cybersecurity broadens our understanding of the potential risks, resonating with our exploration of the potential increase in attacks due to GitHub Copilot misuse.

## VII. CONCLUSIONS AND FUTURE WORK

This study set out to test the hypothesis that GitHub Copilot could generate functional malware from basic user prompts, and the findings compellingly support this assertion. The potential ramifications of GitHub Copilot’s assistive AI technology facilitating the creation of malware by novice programmers and script kiddies with malicious intent are indeed concerning. This technology could spark a surge in new malware, ransomware, and other malicious attacks, particularly within educational and small business environments where physical access to computers is abundant. As novice programmers, budding hackers, and script kiddies oftentimes fall within the age groups that are still attending school, there’s an increased risk of internal attack vectors on educational institutions once they get their hands on these generative AI technologies.

While this discovery poses risks, we should also acknowledge the immense learning potential that generative models like Copilot provide for budding programmers. Efforts to predict and limit the misuse of such technology could be ineffective and ultimately not deter malicious use. The crux lies not in curtailing these powerful tools, but rather in fostering an awareness of their potential misuse, thereby ensuring the responsible adoption of AI technology.

With the rapid evolution and adoption of Large Language Models, it’s incumbent on us to continuously evaluate their potential security implications. Our future work will extend to exploring new AI technologies, programming languages, and various types of malware, ensuring the cybersecurity landscape remains resilient in the face of these emerging challenges.

## ACKNOWLEDGMENT

This work was supported in part by funding from NSF under Award Number OAC 1931363.

## REFERENCES

[1] "Github CoPilot – Your AI pair programmer." Available at: [link](https://github.com/features/copilot).
[2] R. Sabillon, J. Cano M., J. Serra-Ruiz, and V. Cavaller, "Cybercrime and cybercriminals: A comprehensive study," International Journal of Computer Networks and Communications Security, vol. 4, pp. 165–176., 06 2016.
[3] P. Putman, "Script kiddie: Unskilled amateur or dangerous hackers?." United States Cybersecurity Magazine, 2022. Accessed: Dec. 13, 2022.
[4] S. Chng, H. Y. Lu, A. Kumar, and D. Yau, “Hacker types, motivations and strategies: A comprehensive framework,” Computers in Human Behavior Reports, vol. 5, p. 100167, 2022.
[5] PyInstaller Manual - Pyinstaller 5.11.0 Documentation. Available at: https://pyinstaller.org/en/stable/. Last accessed 25-May-2023.
[6] How it works – VirusTotal. Available at: https://support.virustotal.com/ hc/en-us/. Last accessed 1-Dec-2022.
[7] J. Azaria, O. Nakar, and E. Kogan, “The world’s most popular coding language happens to be most hackers’ weapon of choice: Imperva.” Imperva Threat Research, 2022. Accessed: Dec. 14, 2022.
[8] C. Climate, “Code climate documentation.” https://docs.codeclimate. com/, 2022. Accessed: Dec. 10, 2022.
[9] E. B. Martin, “Github copilot: A threat to high school security.” https://github.com/ebsmartin/ GitHub-Copilot-A-Threat-to-High-School-Security, 2022.
[10] M. Botacin, “Gpthreats-3: Is automatic malware generation a threat?,” in 2023 IEEE Security and Privacy Workshops (SPW), pp. 238–254, 2023.
[11] N. Kshetri, “Cybercrime and privacy threats of large language models,” IT Professional, vol. 25, no. 3, pp. 9–13, 2023.
[12] L. Weidinger, J. Uesato, M. Rauh, C. Griffin, P.-S. Huang, J. Mellor, A. Glaese, M. Cheng, B. Balle, A. Kasirzadeh, C. Biles, S. Brown, Z. Kenton, W. Hawkins, T. Stepleton, A. Birhane, L. A. Hendricks, L. Rimell, W. Isaac, J. Haas, S. Legassick, G. Irving, and I. Gabriel, “Taxonomy of risks posed by language models,” in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’22, (New York, NY, USA), p. 214–229, Association for Computing Machinery, 2022.
[13] M. Taddeo, P. Jones, R. Abbas, K. Vogel, and K. Michael, “Socio-technical ecosystem considerations: An emergent research agenda for AI in cybersecurity,” IEEE Transactions on Technology and Society, vol. 4, no. 2, pp. 112–118, 2023
